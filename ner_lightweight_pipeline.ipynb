{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.local/lib/python3.6/site-packages (20.3.3)\n",
      "Requirement already satisfied: kfp in ./.local/lib/python3.6/site-packages (1.1.2)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: Deprecated in ./.local/lib/python3.6/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in ./.local/lib/python3.6/site-packages (from kfp) (1.23.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.1b1 in ./.local/lib/python3.6/site-packages (from kfp) (1.1.2)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.1.2)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: PyYAML in ./.local/lib/python3.6/site-packages (from kfp) (5.3.1)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: strip-hints in ./.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in ./.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.local/lib/python3.6/site-packages (from google-auth>=1.6.1->kfp) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./.local/lib/python3.6/site-packages (from google-auth>=1.6.1->kfp) (3.1.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (45.1.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in ./.local/lib/python3.6/site-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in ./.local/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.25.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in ./.local/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.14.0)\n",
      "Requirement already satisfied: pytz in ./.local/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2020.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in ./.local/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.52.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in ./.local/lib/python3.6/site-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil in ./.local/lib/python3.6/site-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (2.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip --user\n",
    "!pip3 install kfp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import *\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import gcp\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doccano_fomart_to_spacy(data_path):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy==2.0.18'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy download en'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import gcsfs\n",
    "    import plac\n",
    "    import random\n",
    "    import warnings\n",
    "    from pathlib import Path\n",
    "    import spacy\n",
    "\n",
    "    #read from gcs bucket\n",
    "    fs = gcsfs.GCSFileSystem(project='mlops-kubeflow-00')\n",
    "    with fs.open('gs://nerdoc/training_data.json1', 'rb') as f:\n",
    "        data = f.readlines()\n",
    "        print(data)\n",
    "        \n",
    "\n",
    "    training_data = []\n",
    "    for record in data:\n",
    "        entities = []\n",
    "        read_record = json.loads(record)\n",
    "        text = read_record['text']\n",
    "        entities_record = read_record['labels']\n",
    "\n",
    "        for start, end, label in entities_record:\n",
    "            entities.append((start, end, label))\n",
    "\n",
    "        training_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "    with open(f'{data_path}/training_data.txt', 'w') as f:\n",
    "        json.dump(training_data, f)\n",
    "\n",
    "    print('dat saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_path, iterations = 2):\n",
    "    def train_spacy(data_path, iterations = 2):\n",
    "        import sys, subprocess;\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy==2.0.18'])\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy download en'])\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "        #Converting JSON1 file to Spacy tuples format\n",
    "        import json\n",
    "        import numpy as np\n",
    "        import plac\n",
    "        import gcsfs\n",
    "        import random\n",
    "        import warnings\n",
    "        from pathlib import Path\n",
    "        import spacy\n",
    "        import logging\n",
    "        from spacy.util import minibatch, compounding\n",
    "        from spacy.gold import GoldParse\n",
    "        from spacy.scorer import Scorer\n",
    "\n",
    "        with open(f'{data_path}/training_data.txt', 'r') as f:\n",
    "                TRAIN_DATA = json.load(f)\n",
    "                print(TRAIN_DATA)\n",
    "        nlp = spacy.blank('en') \n",
    "        if 'ner' not in nlp.pipe_names:\n",
    "            ner = nlp.create_pipe('ner')\n",
    "            nlp.add_pipe(ner, last=True)\n",
    "\n",
    "\n",
    "        for _, annotations in TRAIN_DATA:\n",
    "             for ent in annotations.get('entities'):\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "        with nlp.disable_pipes(*other_pipes): \n",
    "            optimizer = nlp.begin_training()\n",
    "            for itn in range(iterations):\n",
    "                print(\"Starting iteration \" + str(itn))\n",
    "                random.shuffle(TRAIN_DATA)\n",
    "                losses = {}\n",
    "                for text, annotations in TRAIN_DATA:\n",
    "                    try:\n",
    "                        nlp.update(\n",
    "                            [text],  \n",
    "                            [annotations],  \n",
    "                            drop=0.2,  \n",
    "                            sgd=optimizer,  \n",
    "                            losses=losses)\n",
    "                    except Exception as error:\n",
    "                        print(error)\n",
    "                        continue\n",
    "                print(losses)\n",
    "        return nlp\n",
    "\n",
    "\n",
    "    trainer = train_spacy(data_path, iterations=2)\n",
    "        # Save our trained Model\n",
    "    \n",
    "    trainer.to_disk(f'{data_path}/ner_model')\n",
    "\n",
    "    print('Model saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_spacy(data_path):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy==2.0.18'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy download en'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    import spacy\n",
    "    from spacy.gold import GoldParse\n",
    "    from spacy.scorer import Scorer\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    import argparse\n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import plac\n",
    "    import random\n",
    "    import warnings\n",
    "    from pathlib import Path\n",
    "    import spacy\n",
    "    import gcsfs\n",
    "        \n",
    "    def convert_doccano_fomart_to_spacy(inputs):\n",
    "   # read test data from GCS bucket\n",
    "        fs = gcsfs.GCSFileSystem(project='mlops-kubeflow-00')\n",
    "        with fs.open(inputs, 'rb') as f:\n",
    "            data = f.readlines()\n",
    "            print(data)\n",
    "\n",
    "        training_data = []\n",
    "        for record in data:\n",
    "            entities = []\n",
    "            read_record = json.loads(record)\n",
    "            text = read_record['text']\n",
    "            entities_record = read_record['labels']\n",
    "\n",
    "            for start, end, label in entities_record:\n",
    "                entities.append((start, end, label))\n",
    "\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "        return training_data\n",
    "    \n",
    "    test = 'gs://nerdoc/test_data.json1'\n",
    "    test = convert_doccano_fomart_to_spacy(test)\n",
    "    nlp = spacy.load(f'{data_path}/ner_model')\n",
    "    print('model loaded')\n",
    "    \n",
    "        \n",
    "        \n",
    "    #test the model and evaluate it\n",
    "    examples = test\n",
    "    tp=0\n",
    "    tr=0\n",
    "    tf=0\n",
    "\n",
    "    ta=0\n",
    "    c=0        \n",
    "\n",
    "    for text,annot in examples:\n",
    "        fs = gcsfs.GCSFileSystem(project='mlops-kubeflow-00')\n",
    "        f=fs.open(os.path.join(f'{data_path}/test'+\"result\"+str(c)+\".txt\"), 'w')\n",
    "        doc_to_test=nlp(text)\n",
    "        d={}\n",
    "        for ent in doc_to_test.ents:\n",
    "            d[ent.label_]=[]\n",
    "        for ent in doc_to_test.ents:\n",
    "            d[ent.label_].append(ent.text)\n",
    "\n",
    "        for i in set(d.keys()):\n",
    "\n",
    "            f.write(\"\\n\\n\")\n",
    "            f.write(i +\":\"+\"\\n\")\n",
    "            for j in set(d[i]):\n",
    "                f.write(j.replace('\\n','')+\"\\n\")\n",
    "        d={}\n",
    "        for ent in doc_to_test.ents:\n",
    "            d[ent.label_]=[0,0,0,0,0,0]\n",
    "        for ent in doc_to_test.ents:\n",
    "            doc_gold_text= nlp.make_doc(text)\n",
    "            gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n",
    "            y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]\n",
    "            y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  \n",
    "            if(d[ent.label_][0]==0):\n",
    "                (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
    "                a=accuracy_score(y_true,y_pred)\n",
    "                d[ent.label_][0]=1\n",
    "                d[ent.label_][1]+=p\n",
    "                d[ent.label_][2]+=r\n",
    "                d[ent.label_][3]+=f\n",
    "                d[ent.label_][4]+=a\n",
    "                d[ent.label_][5]+=1\n",
    "        c+=1\n",
    "    for i in d:\n",
    "        print(\"\\n For Entity \"+i+\"\\n\")\n",
    "        print(\"Accuracy : \"+str((d[i][4]/d[i][5])*100)+\"%\")\n",
    "        print(\"Precision : \"+str(d[i][1]/d[i][5]))\n",
    "        print(\"Recall : \"+str(d[i][2]/d[i][5]))\n",
    "        print(\"F-score : \"+str(d[i][3]/d[i][5]))\n",
    "\n",
    "\n",
    "    print('Test completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"2\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "# Create train and predict lightweight components.\n",
    "prep_op = comp.func_to_container_op(convert_doccano_fomart_to_spacy, base_image=\"python:3.7\")\n",
    "train_op = comp.func_to_container_op(train, base_image=\"python:3.7\")\n",
    "test_op = comp.func_to_container_op(test_spacy, base_image=\"python:3.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='ner pipeline',\n",
    "   description=''\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def ner_container_pipeline(\n",
    "    data_path: str,\n",
    "    iterations: int, \n",
    "):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create NER training component.\n",
    "    prep_container = prep_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # Create NER training component.\n",
    "    train_container = train_op(data_path, iterations) \\\n",
    "                                    .add_pvolumes({data_path: prep_container.pvolume})\n",
    "    \n",
    "    # Create NER testing component.\n",
    "    test_container = test_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: train_container.pvolume})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'\n",
    "ITERATIONS=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = ner_container_pipeline\n",
    "experiment_name = 'ner_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"iterations\":ITERATIONS}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
